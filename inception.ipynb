{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script creating class for loading the model which detects the objects (in our project we are\n",
    "interested only in people figure) and coordinates of their boxes\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Class that contains the model and all its functions\n",
    "    code for __init__ and predict methods mostly come from the github repository in the following link\n",
    "    https://github.com/basileroth75/covid-social-distancing-detection/blob/master/src/tf_model_object_detection.py\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"\n",
    "        Initialization function\n",
    "        model_path : path to the model \n",
    "        \"\"\"\n",
    "\n",
    "        # Declare detection graph\n",
    "        self.detection_graph = tf.Graph()\n",
    "        \n",
    "        # Load the model into the tensorflow graph\n",
    "        with self.detection_graph.as_default():\n",
    "            od_graph_def = tf.compat.v1.GraphDef()\n",
    "            with tf.io.gfile.GFile(model_path, 'rb') as file:\n",
    "                serialized_graph = file.read()\n",
    "                od_graph_def.ParseFromString(serialized_graph)\n",
    "                tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "        # Create a session from the detection graph\n",
    "        self.sess = tf.compat.v1.Session(graph=self.detection_graph)\n",
    "\n",
    "    def predict(self, img):\n",
    "        \"\"\"\n",
    "        Get the prediction results on 1 frame\n",
    "        img : our img vector\n",
    "        \"\"\"\n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        img_exp = np.expand_dims(img, axis=0)\n",
    "        # Pass the inputs and outputs to the session to get the results \n",
    "        boxes, scores, classes = self.sess.run([self.detection_graph.get_tensor_by_name('detection_boxes:0'),\n",
    "                                                self.detection_graph.get_tensor_by_name('detection_scores:0'),\n",
    "                                                self.detection_graph.get_tensor_by_name('detection_classes:0')],\n",
    "                                               feed_dict={self.detection_graph.get_tensor_by_name('image_tensor:0'):\n",
    "                                               img_exp})\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def generate_boxes(self, img):\n",
    "        \"\"\"\n",
    "        method generates boxes for the detected objects on the given image/frame\n",
    "        \"\"\"\n",
    "        \n",
    "        # we save the output from the 'predict' method\n",
    "        boxes, scores, classes = self.predict(img)\n",
    "        \n",
    "        boxes_real = []\n",
    "\n",
    "        # we denormalize the coordinates of the boxes and filter them to receive\n",
    "        # only the interesting ones for our project\n",
    "\n",
    "        for i, points in enumerate(boxes[0]):\n",
    "            if classes[0][i] != 1:\n",
    "                continue\n",
    "            if scores[0][i] < 0.7:\n",
    "                break\n",
    "            pts = points.copy()\n",
    "            pts[0], pts[2] = pts[0]*img.shape[0], pts[2]*img.shape[0]\n",
    "            pts[1], pts[3] = pts[1]*img.shape[1], pts[3]*img.shape[1]\n",
    "            \n",
    "            boxes_real.append(pts)\n",
    "        \n",
    "        return boxes_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = r\"models\\faster_rcnn_inception_v2_coco_2018_01_28\\frozen_inference_graph.pb\"\n",
    "model = Model(model_path)\n",
    "\n",
    "# Load image\n",
    "image_path = \"data/people.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Get boxes\n",
    "_, scores, classes = model.predict(image)\n",
    "boxes = model.generate_boxes(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 135.78778,  373.45383, 2190.8926 , 1125.0052 ], dtype=float32),\n",
       " array([ 407.63162, 1568.1224 , 1994.2095 , 2056.3125 ], dtype=float32),\n",
       " array([ 831.68756, 1880.2916 , 2136.1553 , 2592.6099 ], dtype=float32),\n",
       " array([ 640.53925, 2585.6829 , 1840.4769 , 3353.513  ], dtype=float32),\n",
       " array([ 781.86115, 1160.613  , 2106.257  , 1720.8036 ], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the bounding boxes for the detected people\n",
    "boxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human_detection_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
